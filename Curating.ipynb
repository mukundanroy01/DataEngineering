{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17902e92-8cf5-4f9b-84e3-36fa38dc6d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location = \"abfss://<container-name>@<storage-account>.dfs.core.windows.net/Brazil_ECommerce/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649c2b60-4d5f-426e-839d-ff0828d8334c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "raw_schema = StructType([StructField(\"Order_Id\", StringType(), True),\n",
    "                         StructField(\"Order_Status\", StringType(), True),\n",
    "                         StructField(\"Order_Purchase_Timestamp\", StringType(), True),\n",
    "                         StructField(\"Order_Approved_At\", StringType(), True),\n",
    "                         StructField(\"Order_Delivered_Carrier_Date\", StringType(), True),\n",
    "                         StructField(\"Order_Delivered_Customer_Date\", StringType(), True),\n",
    "                         StructField(\"Order_Estimated_Delivery_Date\", StringType(), True),\n",
    "                         StructField(\"Shipping_Limit_Date\", StringType(), True),\n",
    "                         StructField(\"Price\", FloatType(), True),\n",
    "                         StructField(\"Freight_Value\", FloatType(), True),\n",
    "                         StructField(\"Order_Item_Quantity\", IntegerType(), True),\n",
    "                         StructField(\"Product_Id\", StringType(), True),\n",
    "                         StructField(\"Product_Category\", StringType(), True),\n",
    "                         StructField(\"Customer_Id\", StringType(), True),\n",
    "                         StructField(\"Customer_Unique_Id\", StringType(), True),\n",
    "                         StructField(\"Customer_Zip_Code\", StringType(), True),\n",
    "                         StructField(\"Customer_City\", StringType(), True),\n",
    "                         StructField(\"Customer_State\", StringType(), True),\n",
    "                         StructField(\"Seller_Id\", StringType(), True),\n",
    "                         StructField(\"Seller_Zip_Code\", StringType(), True),\n",
    "                         StructField(\"Seller_City\", StringType(), True),\n",
    "                         StructField(\"Seller_State\", StringType(), True),\n",
    "                         StructField(\"Review_Id\", StringType(), True),\n",
    "                         StructField(\"Review_Score\", IntegerType(), True)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b8d082-acec-40e9-b142-b92496a02f20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"mode\", \"DROPMALFORMED\").option(\"header\", True).schema(raw_schema).csv(location+\"Raw/brazil_ecommerce_raw.csv\")\n",
    "df.distinct()\n",
    "df = df.filter(df[\"Order_Status\"] == \"delivered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4770c26c-70b3-443e-9509-26d9c332347d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,datediff,col\n",
    "\n",
    "df_dates_modified = df.withColumn(\"Order_Purchased_Dates\", to_date(\"Order_Purchase_Timestamp\", \"dd-MM-yyy\"))\\\n",
    "    .withColumn(\"Order_Delivered_Customer_Dates\", to_date(\"Order_Delivered_Customer_Date\", \"dd-MM-yyy\"))\\\n",
    "    .withColumn(\"Order_Estimated_Delivery_Dates\", to_date(\"Order_Estimated_Delivery_Date\", \"dd-MM-yyy\"))\\\n",
    "    .drop(\"Order_Purchase_Timestamp\", \"Order_Approved_At\", \"Order_Delivered_Carrier_Date\", \"Order_Delivered_Customer_Date\",\n",
    "          \"Order_Estimated_Delivery_Date\", \"Shipping_Limit_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04178032-44bf-4cb8-b6f5-d2a91f6d6bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_payments = df_dates_modified.select(\"*\", ((df[\"Price\"]+df[\"Freight_Value\"])*df[\"Order_Item_Quantity\"]).alias(\"Total_Payment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0c3722-1907-4cf7-86cb-500a5f8c94bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: True"
     ]
    }
   ],
   "source": [
    "df_payments.coalesce(1).write.format(\"csv\").save(location + \"Curated/Temp\",header = True)\n",
    "\n",
    "filenames = dbutils.fs.ls(location + \"Curated/Temp\")\n",
    "name = ''\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename.name.endswith('.csv'):\n",
    "        name = filename.name\n",
    "\n",
    "dbutils.fs.cp(location + \"Curated/Temp/\" + name, location + \"Curated/brazil_e_commerce_curated.csv\")\n",
    "dbutils.fs.rm(location + \"Curated/Temp/\",recurse = True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Curating",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
